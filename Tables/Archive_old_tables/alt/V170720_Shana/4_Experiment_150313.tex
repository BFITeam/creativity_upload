

%Dutcher 13 JEBO nice and concise description of unusualuses task

%Colin Martindale (1999, "The Biological Bases of Creativity") hat Poincaré ein ähnliches Zitat in dem Buch "The Foundations of Science" (1913, Seite 286, Lancester, PA: Science Press) benutzt. Das Zitat lautet "To create consists of making new combinations of associative elements which are useful".

In the following, we introduce the 
experimental tasks, the set up, the treatments, and the experimental procedures. 

\subsection{The Tasks}\label{sec:tasks}
In order to assess the effectiveness of rewards for routine and
creative tasks, we implemented both types of tasks in the experiment.

We use the ``slider task'' as a proxy for simple, routine tasks in the workplace \citep{2012_Gill_Prowse}.
The slider task is a real effort task that has a number of desirable attributes. It is easy to explain and 
to understand, and it does not require prior knowledge. 
It is identical across repetitions, involves little randomness, and leaves no scope for guessing. 
The task features a computer screen displaying 48 sliders on scales that range from
0 to 100. Figure \ref{fig:screenshot} shows an example of the screen as it 
was presented in the experiment. 

Initially, all sliders are positioned at zero.
The aim of the task is to position as many sliders as possible at exactly 50 within 3 minutes
by using the mouse. Keyboards were disconnected during the task to prevent 
the usage of the arrow keys. Each slider can be adjusted and re-adjusted an unlimited number of times. 
While moving the mouse, subjects cannot be sure whether they positioned the slider at exactly 50. 
The exact position of the slider is displayed to the right of the scale only  when the subject 
stops using the mouse. We measure a subject's performance as the number of correctly positioned sliders within the alotted time. Gill and Prowse demonstrated that this measure corresponds closely to the effort exerted by a subject. 
Before the start of Period 1, subjects were given one minute to practice the task.

%\begin{figure}[H]
%\caption{Screenshot of the Slider Task}
%\begin{center}
%\includegraphics[width=0.7\textwidth]{Paper3_Graphs/screenshot_slidertask.jpg}
%\label{fig:screenshot}
%\end{center}
%\end{figure}

We measure creative performance via the ``Unusual Uses Task.''
Originally developed as Guilford's Alternative Uses Task \citep{Guilford67}, it was later 
 incorporated in the \textit{Torrance Test of Creative Thinking} \citep{Torrance1968,Torrance1998}, 
the most widely used and validated test to assess creativity \citep{kim2006}. In our implementation, we follow closely the procedures that psychologists have developed over many decades.
%\footnote{In order 
%to assess overall creative potential, the \textit{Torrance Test of Creative Thinking} 
%also includes a number of figural elements that require drawing skills.
%The more specific Unusual Uses task, however, best captures the type of creativity that we want to study. 
 %}
In the Unusual Uses Task, participants are asked to name as many, unique and unusual uses for an ordinary item, such as a tin can, as they can. 
This captures a central element in applied business
innovations: the recombination of existing bits of knowledge in novel 
ways \citep{Weitzman1998, woodman93, Simonton2004}.
%\footnote{Different types of creativity 
%economists have looked at include word formation from a given letter set 
%\citet{Eckartz2011}, open problem solutions \citep{charness12WP} or exploration 
%in a computer simulation game \citep{Ederer12MS}.}
Specifically, the task requires divergent thinking or ``thinking outside the box,''
which is one of the most important components of the creative process \citep{runco1991divergent}. 
One advantage of the Unusal Uses task for our purposes  is that it provides a clean numerical 
measure of creative productivity. 

% XXX Text aus Azoulay et al 2009: 
% "Some discoveries are incremental in nature, and reflect the fine-tuning of previously available technologies, or 
% the exploitation of established scientific trajectories. Others are more radical and require the exploration
% of new, untested approaches. Both forms of innovation are valuable, but we still have a
% poor understanding of what drives radical innovation. One view is that radical innovation
% happens by accident. From Archimedes' eureka moment to Newton's otherworldly contemplation
% interrupted by the fall of an apple, luck (and sometimes talent) play an essential
% role in lay theories of breakthrough innovation. Of course, if luck and talent exhaust the
% list of ingredients necessary to produce breakthroughs, then there is little for economists to contribute.
% As argued by Manso (2009), however, incentives may play an important role in the production
% of ideas, particularly novel ones. In this paper, we build upon his theoretical framework
% to provide empirical evidence that nuanced features of incentive schemes embodied in
% the design of research contracts exert a profound influence on the subsequent development
% of breakthrough ideas

% XXX Vorschlag was wir zur Thematik radical vs incremental reinnehmen u gleichzeitig Bezug zu 
% Qualit�ts-Quantit�ts-trade-off machen k�nnen
% Creative performance in this task captures both, quality and quantity of ideas. 
% In contrast to radical innovations, which are known to be sometimes the result of 
% fortunate coincidences, incremental innovations are triggered by the production of many ideas.
% Based on a simple statistical relationship, the more ideas are produced, the 
% larger the odds that one or more of these ideas are implemented successfully \citep{Simonton1999}.

In the experiment, subjects had to sequentially brainstorm unusual uses for three different items:
a sheet of paper, a tin can, and a cord.
Subjects were informed that they should not limit themselves to a 
particular size of the item. Moreover, the unusual use that they come up with could 
require more than one of the items; for instance,
the use could require more than one sheet of paper or several tin cans.
%\footnote{The precise wording can be found in the Appendix.}
The order in which subjects had to work on the items was fixed, regardless of treatment: (1) paper, (2) 
 tin can, (3) cord.%\footnote{Controlling for order effects is not
%important in our design as we use the same order in the control group and
%only look at changes in performance between periods and between treatment and control
%groups.}

In the creative task, just as in the slider task, subjects had a test period of one minute. 
In this test period, subjects were given the item ``old tire'' to familiarize themselves with 
the task and the input mask on the screen.
We used the three standard measures of the Unusual Uses task to evaluate subjects' responses:
fluency, flexibility, and originality \citep{Guilford1959}, and we told subjects how their answers would be scored.\footnote{The original Guilford Test 
uses a fourth criterion for scoring, elaboration, which refers to the degree of detail
of the answers. We refrained from using this fourth dimension because it is largely 
effort-based and would have constrained our capacity to score answers 
within the time frame of the experiment.} Fluency refers to the number of valid answers. 
An answer is valid if the stated use is possible to implement 
and the realization is at least vaguely conceivable.
Fantastic or impossible uses are not counted. 
Examples of a valid use of a tin can are, for instance, a flower pot, 
a pen container, and a drum. In contrast, examples of invalid answers are 
the use of a tin can as a television, a computer, or a window.\footnote{Usual uses, 
such as a food  container in the case of the tin can, were not scored in the original version of 
Guilford's Alternative Uses Task or the TTCT. However, the original instructions of the 
test (as well as our instructions) do not explicitly exclude usual uses from scoring. 
We therefore scored usual uses as valid answers. Excluding usual uses, however, does 
not alter any of the results reported below (results available upon request).}
In the experiment, each valid use was given one point.

The second evaluation measure, flexibility, reflects the variety of a
subject's responses and is determined by counting the number of different categories into which 
responses fall. For instance, the answer candleholder falls into the category `decoration', 
 %answers like cookie cutter or sand toys into the category `forms,' 
% Ausstechform Form/Schablone Form Form/Schablone Kerzenform Form/Schablone Kreise zeichnen Form/Schablone Kuchenform Form/Schablone Sandkastenform Form/Schablone Schablone %Form/Schablone Ausstechform Form Kerzenform Kreise zeichnen Kuchenform Sandkastenform Schablone 
%XXX check the englishXX, 
and  answers like a rattle or a 
drum into the category `musical instruments.' 
Subjects received one point for each category. 
Common categories for the tin can include `non-food containers' 
(for instance, a pen container), `sporting goods' (for instance, a football), 
and `communication' (for instance, a tin can phone).  Overall, there were roughly 
55 categories for each of the three items that we used. 
 
Finally, the originality of responses was measured by the statistical infrequency of answers.
 In order to get an idea of the frequency of responses, 
we conducted a pre-test with 127 participants who worked on the three 
items under a fixed wage scheme. 
We then tabulated all valid answers for each item according to the frequency 
with which the answer was given and constructed a rating scale to assess 
answers in the experiment. This scale allotted one additional point to a 
valid answer if less than 8\% (``original'') and two additional points if less than 1\% (``very original'') of participants gave
 that answer. In comparison to other measures of creativity that rely on expert ratings, 
our  statistical approach to originality is more reliable and objective.\footnote{The answers gathered during the experiment itself allowed 
us to update the rating scale to more accurately reflect overall statistical infrequency. 
The rating scale used for analyzing our results is based on more than 
   700 subjects (pre-test subjects as well as subjects in all main treatments including the
 supplementary \textit{Feedback} treatment.) The results do not depend on 
whether or not we use an updated or the original, pre-test rating scale. Finally, our results are robust 
to using an expert panel for grading originality rather than the statistical approach (details  below). }
 Examples for an original use of a tin can are an insect trap or an animal 
house. Very original answers include using the tin can as a scarecrow, 
a shower head, a treasure chest, or a grill (by putting coal into it and meat on top). Table \ref{tab:paper3_Examples} illustrates 
further examples of frequent answers and categories as well as original, very original, 
and invalid responses for all items. 

 


%it belonged to the 8\% most rare answers
%in the pre-test (``original'') and two points to answers that belonged
 %to the 1\% most rare answers in the pre-test (``very original'').
% However, infrequently used words may not necessarily be
% useful and original. Therefore, we used as a robustness check another scoring of the
% originality of the answer given by rating each answer subjectively. Therefore two evaluators,
% who were blind to the treatment interventions, rated the answers.
% These pre-test answers were used in the experiment to assess originality and compensate subjects.
% For the analyses reported in this paper, we use originality scores 
% which are based upon all responses elicited in the experiment and the pre-test 
% to have an even more comprehensive measure of the frequency of particular answers,
% and hence, their originality. 

Scoring was conducted by research assistants who were well versed with the scoring procedures and blind to the treatments. 
%However, some scope for subjectivity remains 
%in the evaluation of the unusual uses task, a feature typical of creative tasks. 
%To be able to link our study to the debate in psychology about creativity and intrinsic motivation, we had 100 subjects work on both tasks and rate their interest in the tasks on  Likert-scales from 1 to 7. %\footnote{Subjects of the sudpplementary treatments \textit{SCS} and \textit{CSC} were asked for this assessment 
% as they experienced both tasks. However, as these treatments also included a tournament in one of 
% the tasks, we only use the assessment of the task which was not rewarded according to performance as 
% this might bias subject's judgment.} 
%According to this assessment, the creative task was rated as statistically significantly 
%more interesting than the routine task (Wilcoxon rank-sum test, p=0.02).
The  point system that we use is the same as the one used in the Torrence Test
of Creative Thinking. To economists, this point system will appear somewhat arbitrary. We will therefore report the results of each evaluation measure separately as well. 

 It is important to note that while the Unusual Uses task has many desirable attributes
 for the purposes of our study, it is not without limitations from an economist's perspective. 
In particular, it  captures a particular kind of creative process, ``divergent thinking'' and it is unclear whether 
our findings carry over to tasks that require ``convergent'' thinking, that is, 
tasks that require finding one unique solution to a problem.\footnote{A prime example of 
divergent thinking are brainstorming sessions where one tries to come up with has many possible 
solutions to a problem as possible. Convergent thinking, by comparison, refers to situation in which there is only one
or a very limited set of possible solutions and one tries to find it/them. Examples include algorythms or chemical 
substances with certain properties. \cite{kim2006} for a brief discussion of the Torrence Test and its relationship to
convergent and divergent thinking.
} One could, in fact, 
argue that the task is a complex task and that there is no guarantee 
that a high score in the task actually reflects true creativity in every case (e.g., \citealp{kim2006}). 
Also, the task measures and rewards both 
quantity and originality. In many real-world settings, the principal cares about 
the ``best'' idea and not about many, original ideas. To the extend that one wants to learn 
about such situations, one can consider creativity as one input to 
developing the ``best'' idea, and this paper sheds light into how this input responds to the provision 
of explicit and implicit performance incentives. 
Finally,  in order to compensate agents, we used a particular formula that gave numeric 
weights to the different dimensions. This formula is taken directly from the Torrence Test of 
Creative Thinking, but there is no particular reason why those 
weights should be the ``correct'' ones from the perspective of the principal. 
In other settings, this metric might look differently, might be less precise, 
and have different weights. For simplicity, we report our 
main results in terms of this numeric score, but we also report the performance in the individual dimensions.   

%there is a clearly defined scoring metric for your creative task. The metric that the task uses as well as the weights given to the individual components (validity, fluency, and originality) are clearly specific to the task and our setting. In other settings, the metric might look differently, might be less precise and have different weights. 





\subsection{Basic Set up \label{sec:groups}}
The experiment uses a principal-agent set up, where subjects are randomly 
assigned to the role of a principal (``employer'') or an agent (``employee'').
This feature is important for reciprocity considerations as it allows
1) for voluntary financial transfers from the principal to the agent, and 
2) agents' effort to affect the principal's payoff, which gives agents a clear way of reciprocating if they wish to do so. 
%\footnote{We refrained from a set-up where rewards are implemented 
%by the experimenter as it is hard for subjects to interpret unconditional
% transfers by the experimenter. Even if such a transfer were perceived as `kind',  subjects 
%do not know the payoff function of the experimenter and, hence, wouwill not reciprocate \citep{Hennig2010, Englmaier10WP}.}
At the start of the experiment, subjects were assigned to 
groups of five participants, each consisting of one principal 
and four agents. The role and group assignment remained fixed 
throughout the experiment. 
% Within one session, all groups worked on the same task and were confronted with the same treatment.\footnote{In 
% two additional treatments, which we will introduce in Chapter \ref{chap:ex_post}, we varied the tasks an agent was confronted with.}

All sessions were identical in their basic structure: 
employees were asked to work for the principal for three 3-minute periods on either the routine or the creative task. Treatments (see below) were introduced before the second working period. Figure XXX depicts this
basic set up. 
The three-period design allows us to measure agents' baseline performance under a fixed wage in Period 1,
% -- joint measure of intrinsic motivation for the task, their altruism towards the principal, and a subject's ability --
the performance response to the reward in Period 2 -- our main focus of interest -- and post-treatment 
performance under a fixed wage in Period 3 -- a joint measure of any post-treatment effects related to, for example, 
treatment specific learning or lasting changes in intrinsic motivation. In order to keep the discussion of the results 
short and concise, we have relegated the discussion of period 3 effects to the Appendix. 
%, the tournament incentive. Otherwise, there would be a positive response to the tournament incentive in 
%Period 2, and a reduction in intrinsic motivation -- if permanent -- can be detected in Period 3, 
%where the tournament reward is no longer at stake \citep{Frey01JES,bowles12JEL}. Unlike other models of crowding out, in Benabou and Tirole's (2006) \nocite{Benabou06AER} model, crowding out only persists as long as the incentives are in place.  Crowding out that is temporary and tied to the presence of incentives cannot be directly identified in our design.  Most discussions of crowding out (e.g. in the education literature) focus on the permanent crowding out as a consequence of temporary incentives.  It is that phenomenon that we attempt to capture in this paper.} 
%Period 3 is important as detrimental effects of rewards on future intrinsic 
%motivation cannot necessarily be detected when rewards are present as the increase in monetary incentives might outweigh the reduction in intrinsic motivation. 

%Therefore, we observe performance in period 3 
%to measure potential long-term effects of rewards on intrinsic motivation. 
%\footnote{Potential detrimental effects to intrinsic motivation can nevertheless be addressed with second period data by comparing the performance response to the reward between the creative and the simple task as intrinsic motivation is smaller or non-existent in the latter. }
After the three working periods, agents completed a few brief decision tasks as well as
%which included incentivized control measures for subjects' risk and tournament aversion 
%social preferences. 
questions 
about their  socio-demographic characteristics such as  gender, field of study, 
level of education, high school grade and leisure activities as well as questions regarding 
their personality traits.
%, including the Big Five traits.\footnote{Risk aversion was elicited with 
%a simple multiple price list design which allows us to infer subjects'
%certainty equivalent for a given lottery (see, for instance, \citealp{Bradler2009}).
%Tournament aversion was observed with a 3-minute counting task (see \citealp{Abeler2011}) where subjects
%could choose between a piece rate and a tournament reward scheme.
%Additionally, we implemented a measure for the social preference type \citep{Murphy2011} 
%as well as people's reciprocal inclinations in a simple gift-exchange game similar to \cite{Englmaier10WP}. 
%For each incentivized control measure subjects were informed 
%that they would interact with a different randomly matched subject. At the end of the experiment, 
%one of these four incentivized measures was randomly chosen for payoff. The personality scales 
%that we used were the Big Five and reciprocity questions from the Socio-Economic Panel (see \citealp{Gerlitz2005}) as well as
% the Gough scale, a validated scale that measures creative personality with a 30-items 
%adjective check list \citep{Gough1979}.} 
%Furthermore, we asked subjects whether friends of 
%them participated in the same session, whether they thought that they performed above or below 
%average compared to other subjects in the session (treatments without performance evaluation only)
% and how much they think their principal earned in the experiment (without fixed wage).






Employers' payoffs consisted of a fixed  pay
component and a variable pay component that was determined by the 
performance of the four agents in their group. All payoffs during 
the experiment were stated in ``Taler,'' the experimental currency unit. 
The exchange rate was 100 Taler = 1 Euro. 
In the routine task, principals received 5 Taler for
each slider that was correctly positioned by their four agents.
In the creative task, principals received 5 Taler for each
validity point (valid answer), 5 Taler for each flexibility point 
(category mentioned), and 5 Taler for each originality point 
(5 Taler per original answer and 10 Taler per very original answer) given 
by their four agents. Agents learned about the scoring procedures  and the principals' payoff function in the instructions. 
%\footnote{Creative answers were scored irrespective of whether or not
%the same answer was also given by another agent in the group. A maximum
%of 20 Taler could be earned per idea, 5 Taler for the idea being valid, 5 additional
%Taler when the idea fell into a new category and 10 additional Taler if the
%idea was very original. A pre-test gave us an idea about
%the productivity of agents in both tasks. The payoffs were calibrated in a way
%that ensured approximately equal pay-offs for both agents and principals across
%sessions with different tasks.}

In order to create an environment that carried an opportunity cost of working, we offered agents 
a \textit{time-out button} \citep{Mohnen08} which was displayed at the bottom 
of the screen during all working periods. Each time an agent clicked the 
time-out button, the computer screen was locked for 20 seconds, 
prohibiting the entry of creative ideas or the movement of sliders, and 5 Taler
were added to the agent's payoff. This procedure has been used in a variety of experiments
to ensure that experimental subjects do not merely work on the experimental tasks
out of boredom due to the absence of alternative activities \citep{Eckartz2011,Mohnen08}. 
Subjects could push the time-out button as long as the remaining time in the working period was at least 20 seconds. 
In order to ensure that subjects were aware of the time-out button and
understood its usage, we had a trial period that lasted 60 seconds in which subjects could test the time-out button. While the time-out button prevents any ``production'' in the simple, routine task, we cannot rule out that subjects in the creative tasks continued thinking about the problem during the time that their screen was locked. This does not mean, however, that subjects in the creative task did not face any opportunity costs of time.  The timeout button may still hinder production since it precludes a critical task -- entering ideas.   Overall, the use of the time-out button was limited, suggesting that research subjects felt their time was better spent completing the assigned task. XXXXX additional info: how often it was used etc etc XXX


% An overview of our main control variables across treatments and tasks is
% provided in the Appendix in Table \ref{tab:Summary_Statistics}.


\subsection{Design and Implementation of Treatments}


%XXX from above
%In each period, employees received a fixed wage that was exogenously set by the experimenter 
%and announced at the start of the respective period. In the two reward treatments, principals
%could opt for or against a financial gift or a tournament before the start of Period 2 (details below). 
%In the \textit{Gift} treatment, Period 2 endowments were augmented by the amount of the gift when the principal opted for the gifts. In the \textit{Tournament} treatment, agents could receive a tournament prize at the end of Period 2 if the principal opted for the tournament scheme and if their performance was above average. 
%In all treatments, Period 3 was identical to Period 1 in that endowments were fixed and there were no additional rewards or tournaments. 
%XXX


In order to address our research questions,
we implemented a 2 x 3 design consisting of a \textit{Control} group, a \textit{Tournament} treatment, and
a \textit{Gift} treatment for both the routine and the creative task. We used a performance tournament rather than a 
piece rate as the performance-dependent reward scheme   because tournaments 
are widely used in practice  to reward individuals for creative performance and innovations \citep{Brunt2012,Kremer2010}. 
For instance, companies increasingly allocate creative tasks to online platforms with creative 
contests (such as www.innocentive.com or www.jovoto.com) to complement their 
in-house research and development. These platforms offer tournament-based 
compensation for various creative tasks such as scientific problem-solving, 
software development, and graphic art design \citep{Boudreau2011}.  


In the \textit{Control} group, agents were paid a fixed wage in each period and 
 principals were not able to implement  rewards. The control group (one per task)
allows us to account for learning 
and fatigue, and to standardize performance across the two tasks (so that we can compare effect sizes). 

In the \textit{Tournament} and the \textit{Gift} treatments, principals and agents were 
informed at the end of period 1 that the principal could  invest in a reward scheme for period 2. 
Regardless of whether the principal decided to implement the reward scheme, agents received information on the type of the reward (tournament or gift, depending on the treatment)
 and on the associated costs to the principal. 
Before the start of Period 2, agents learned whether their principal had instituted the reward.  
Subjects were also told that principals did not receive any 
information about their agents' performance until the very end of the experiment. 
This ensured that agents perceived the wage gift as ``kind,''  rather than as 
compensation for good performance in Period 1. Moreover, it avoided an endogenous 
selection of rewarded agents based on Period 1 performance.

In both treatment groups, principals and agents 
received a fixed wage of 300 Taler at the beginning of each period. 
In the \textit{Gift} treatment, the principal had to decide whether or not to 
provide an additional monetary gift of 300 Taler to each of her four agents at a total cost of 200 
Taler to herself.\footnote{The use of  efficiency factors is common practice 
in the experimental literature on gift exchange 
(see, for instance, \citealp{Brandts2004}) and is thought of as representing situations in which 
gifts  are more valuable to the recipient than to the donor.  
The attractiveness of the reward was important in 
our setting because we were mainly interested in agents' 
responses to rewards rather than in whether or not principals opted for the rewards.
} 
In the \textit{Tournament} treatment, the principal 
could also transfer a  total of 1200 Taler to her four agents at a  cost of 
 200 Taler to herself. This feature allows us to draw conclusions about which of the 
two reward schemes is prefereable given a fixed budget for rewards. 
However, the payment structure was different in the tournament than in the gift treatment.  
In the tournamnet treatment, agents' performance dictated whether or not they received a 
reward. Specifically, the top 50\% of performers (two out of four agents) received a 
bonus of 600 Taler each in Period 2, whereas 
the bottom 50\% received nothing.  Subjects in the \textit{Control} group also received a fixed 
wage of 300 Taler in Periods 1 and 3, and a fixed wage of 600 Taler in Period 2. This increase 
in Period 2 mirrors (expected) payoffs in the treatment groups. This ensures that any performance difference between 
the treatment and the control groups is driven by the treatments rather than by endowment effects.\footnote{It is possible
that the higher pay in the control group in period 2 might induce a reciprocal response (towards the experimenter). 
A priori it is unclear, however, whether reciprocity should increase or decrease performance (increased performance might be considered 
desirable but leads to  additional expenses for the experimenter because he needs to pay the variable pay 
component to the principal). \cite{Hennig2010} documents that wage gifts are not reciprocated when the payoff 
function of the experimenter is unknown to subjects, which is the case here. 
Similarly, the institution of the tournament could induce a reciprocal response (towards the principal).
To the extent that this is true, the  tournament effect -- just like any tournament effect in the literature -- comprises 
both the incentive effect of the tournament and the reciprocity that goes hand in hand with it. } 
In essence, therefore, the two treatment groups and the control group are budget equivalent ways of compensating workers
 and the results will show which way of compensating workers results in the highest output.

When learning about the principal's reward decision, employees  also learned that performance
would be evaluated immediately after Period 2 ended, and that the winners and 
losers of the \textit{Tournament} 
would be revealed before Period 3 started. Finally, after Period 2 and after the revelation 
of winners and losers (presented as private information on a subject's screen), 
subjects in both treatments  were 
informed that there would be no further rewards.  In all treatments (including the control group) it 
was further announced that the payment structure in Period 3 would be identical to that  in Period 1.\footnote{The tournament treatment effect is a reduced form that captures a range of possible channels via which worker output is affected. For one, tournaments give workers strong incentives, which may increase effort. Also, tournaments might be perceived as a gift from the principal (it does cost the principal money). To the extent that this is true, the tournament treatment effect also reflects the combination of incentives, reciprocity, and any other factor that might be at work.  }

This study  focuses on agents' responses to rewards, rather than on 
principals' reward decisions. Therefore, the rewards were relatively cheap for the principal, and 
endowments in the control group mirrored 
(expected) payments in the two treatment groups when the principal opts for the reward. Specifically,
endowments in Periods 1 and 3, periods without rewards, are identical in the 
control and the two treatments groups:
the principal as well as each of her four agents receive 300 Taler.\footnote{On top of their endowment, principals earn additional money from the performance of their agents.}
In Period 2, principals in the control group receive an endowment of 100 Taler and agents an 
endowment of 600 Taler each. This  mirrors the expected payoffs in the treatment groups when the principal opts for the reward scheme (unconditional gift or tournament, depending on the treatment).\footnote{If the principal decided against the reward, the principal and her agents 
received 300 Taler each as fixed Period 2 endowments 
in both the \textit{Gift} and \textit{Tournament} treatments (identical to payments in Periods 1 and 3), 
while they 
earned 100 and 600 Taler in the 
control group, respectively. Therefore, we 
cannot assess responses to negative reward decisions in an experimentally clean way.}
This procedure ensures that any performance differences between the treatments and the 
control goup are solely driven 
by the rewards and not by other factors such as distributional concerns 
or income effects.\footnote{See, e.g., 
\cite{Fehr1999} on inequality aversion. Our set up also disentangles  
pure intention-based reciprocity from other distributional concerns \citep{Charness2004}
by allocating the same payoffs to subjects in the treatment groups (with positive reward decisions)
 and the control group in Period 2. The only difference that remains is that control group  payoffs were
exogenously imposed by the experimenter, while treatment payoffs were 
chosen by experimental subjects, the principals. } 
Table \ref{tab:Pay_Off_Design} in the Appendix provides an overview of the fixed and 
variable pay components for all periods, treatments, and roles. 

%\subsection{Hypotheses}\label{sec:hyp}

%Before we present our results, we briefly summarize the theoretical predictions for our treatments. 

%In general, economic theory tends to abstract from the type of 
%task and therefore predicts uniform effects across our two tasks. Theories on crowding out of intrinsic 
%motivation offer differentiated hypothesis for tasks that are high and low 
%in intrinsic motivation, in our case, the creative and the simple task. As the routine task 
%is relatively low in intrinsic motivation, theories on crowding-out 
%effects either should do not apply, or  should only apply to a negligible extent, to this task.
%Arguments on a crowding-in effect, however, have not been restricted to tasks which 
%are already high in intrinsic motivation, but apply to both tasks \citep{Eisenberger1999}.
%}  

%\textit{Tournament} treatment. 
%According to standard economic theory, a tournament incentive should increase average 
%effort and performance \citep{Alchian72AER,Lazear81JPE}. Since economic theory tends 
%to abstract from the type of task, this should be true in both the creative and the routine task. 
%Theories on crowding-out, on the other hand, suggest that intrinsic motivation 
%might be impaired by the reward (e.g., \citealp{Deci1972, Lepper73JPSP, Deci99PB}). 
%This holds in particular for the creative task that is more intrinsically motivating. 
%When crowding out is strong, the reduction in intrinsic motivation would outweigh the incentive effect 
%and Period 2 performance would fall. Otherwise, there would be a positive response to the tournament incentive in 
%Period 2, and a reduction in intrinsic motivation -- if permanent -- can be detected in Period 3, 
%where the tournament reward is no longer at stake \citep{Frey01JES,bowles12JEL}.\footnote{Unlike other models of crowding out, in Benabou and Tirole's (2006) \nocite{Benabou06AER} model, crowding out only persists as long as the incentives are in place.  Crowding out that is temporary and tied to the presence of incentives cannot be directly identified in our design.  Most discussions of crowding out (e.g. in the education literature) focus on the permanent crowding out as a consequence of temporary incentives.  It is that phenomenon that we attempt to capture in this paper.} 

%\footnote{The aforementioned opposing camp in the literature on intrinsic motivation, the \textit{crowding-in} literature suggests that performance-dependent 
%rewards might  increase feelings of self-determination and thus intrinsic motivation and 
%performance \citep{Eisenberger1999,Eisenberger96AP}. These arguments 
%should apply for both tasks. Such an increase in performance in period 2 is not 
%distinguishable from the positive incentive effect according to standard economic theory. Again, only period 3 will allow us to distinguish whether a possible increase in period 2 performance is %driven by the incentive effect of the reward or an increase in intrinsic motivation: Crowding-in theory predicts a persisting 
%positive effect on intrinsic motivation, and thereby on performance, in period 3, while 
%standard economic theory assumes that performance returns to the baseline 
%level in period 1 (controlling for any changes due to learning and exhaustion that we measure in the control group). } 

%\textit{Gift} treatment. 
%Established theories on reciprocity predict that 
%agents  reciprocate  principals' gifts with increased effort 
%\citep{Akerlof1982,Levine1998,Dufwenberg04GEB,Falk06GEB}. 
%Thus, we expect a positive performance response to the gift in both tasks. 
%Depending on how 
%sustainable these responses are, this effect might 
%carry over to Period 3.\footnote{Theoretically, agents could also reciprocate 
%the investment in the tournament. We will address this point in the results section. }
%\footnote{Again referring to \citet{Eisenberger1999,Eisenberger2001}, 
%there is the alternative prediction that performance-independent rewards might 
%have a detrimental effect on performance 
%because they are perceived as rewarding ``average'' or inadequate performance, which 
%in turn might impair intrinsic motivation.  
%These arguments hold in particular for the creative task as intrinsic motivation is 
%higher there.}


%Both camps, however, agree that positive feedback information enhances feelings of competence, and thus intrinsic motivation, while  negative performance feedback rather diminish the latter \citep{Reeve1996,Eisenberger1999, Vansteenkiste03}. Thus, crowding-out effects might be lower for recipients of positive performance feedback. We are able to disentangle this.

% evtl noch einen Satz zum wording:
% In terms of the wording, we told the worst 50\%, in both the tournament and the feedback treatment: "With your
% performance you do not belong to the best 50\%" instead of telling them that they belong to the worst 50\%.




\subsection{Procedures}
The experiment was conducted at the experimental laboratories at the universities
of Frankfurt, Mannheim, and Heidelberg, Germany.
%\footnote{In Mannheim, experiments took place in the experimental laboratory of the 
%"SonderForschungsBereich 504", in Frankfurt at the "Frankfurter Labor f{\"u}r Experimentelle Wirschaftsforschung"
%(Frankfurt laboratory for experimental economic research, FLEX) and in Heidelberg at the
%AWI Lab of Experimental Economics.}
Participants were recruited via the Online Recruitment System ORSEE
 \citep{Greiner2004}. 
%We ran 67 sessions with 750 subjects between April and September 2012.
%350 of the subjects worked on the routine and 400 on the creative task.\footnote{The higher number of
%subjects for the creative task is driven by a larger percentage of principals
%that decided against reward implementation in this task (see Table \ref{tab:Summary_Statistics}).}
The experiment was computerized using the software z-Tree \citep{fischbacher1999z}. 
%In order to ensure a balanced number of subjects in our analysis across treatments,
%we ran experimental sessions until we had approximately 60 agents
%with reward implementation by the principal in each treatment. Therefore, 
%the exact number of total subjects differs somewhat between treatments.
%Table \ref{tab:Summary_Statistics} provides an overview of the descriptive
%statistics, including the number of subjects per treatment.

All interactions within the experiment were anonymous and communication was not allowed.
Subjects were seated randomly at a computer workstation upon arrival and were 
provided with hard copy instructions that detailed the random matching of groups and roles
 (``employer'' or ``employee''), the basic structure of the experiment, the  task (routine or creative), 
and the scoring procedures. 
A translation of the original instructions can be found in  the Appendix.
A few pieces of information, such as fixed wages in Periods 2 and 3 as well as 
the availability of rewards and reward decisions, were presented on the computer screen 
during the experiment.
Before the experiment started, subjects had to complete  a series of questions about 
how their actions would determine their own and their principal's payoffs 
to ensure that they understood the instructions. \\ %Questions were answered in private.
%\footnote{As principals' only active role in the experiment was the reward decision, 
%they were offered the opportunity to work on the task as well in order to avoid 
%boredom. However, they were informed that their payoffs were not influenced by their own performance.}
At the end of the experiment, subjects' payoffs in the experimental
currency unit ``Taler'' were converted into Euros at an exchange rate of 100 Taler = 1 Euro. Subjects were paid in private.
Sessions (including instructions, the experiment, and questionnaires) lasted  about 75 minutes and subjects earned 15 Euros on average. 


